{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kbosko/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/kbosko/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import re\n",
    "import warnings\n",
    "import pickle\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet'])\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "#, FeatureUnion, make_pipeline, make_union\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "#, make_scorer, accuracy_score, jaccard_score, hamming_loss\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///../data/DisasterResponse.db')\n",
    "df = pd.read_sql(\"SELECT * FROM DisasterResponse\", engine)\n",
    "X = df['message']\n",
    "Y = df.iloc[:, 4:]\n",
    "category_names = df.columns[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message  \\\n",
       "0   2  Weather update - a cold front from Cuba that c...   \n",
       "1   7            Is the Hurricane over or is it not over   \n",
       "2   8                    Looking for someone but no name   \n",
       "3   9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
       "4  12  says: west side of Haiti, rest of the country ...   \n",
       "\n",
       "                                            original   genre  related  \\\n",
       "0  Un front froid se retrouve sur Cuba ce matin. ...  direct        1   \n",
       "1                 Cyclone nan fini osinon li pa fini  direct        1   \n",
       "2  Patnm, di Maryani relem pou li banm nouvel li ...  direct        1   \n",
       "3  UN reports Leogane 80-90 destroyed. Only Hospi...  direct        1   \n",
       "4  facade ouest d Haiti et le reste du pays aujou...  direct        1   \n",
       "\n",
       "   request  offer  aid_related  medical_help  medical_products  ...  \\\n",
       "0        0      0            0             0                 0  ...   \n",
       "1        0      0            1             0                 0  ...   \n",
       "2        0      0            0             0                 0  ...   \n",
       "3        1      0            1             0                 1  ...   \n",
       "4        0      0            0             0                 0  ...   \n",
       "\n",
       "   aid_centers  other_infrastructure  weather_related  floods  storm  fire  \\\n",
       "0            0                     0                0       0      0     0   \n",
       "1            0                     0                1       0      1     0   \n",
       "2            0                     0                0       0      0     0   \n",
       "3            0                     0                0       0      0     0   \n",
       "4            0                     0                0       0      0     0   \n",
       "\n",
       "   earthquake  cold  other_weather  direct_report  \n",
       "0           0     0              0              0  \n",
       "1           0     0              0              0  \n",
       "2           0     0              0              0  \n",
       "3           0     0              0              0  \n",
       "4           0     0              0              0  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map between Treebank and WordNet \n",
    "# WordNet POS tags are: NOUN = 'n', ADJ = 's', VERB = 'v', ADV = 'r'\n",
    "# Descriptions (c) https://web.stanford.edu/~jurafsky/slp3/10.pdf\n",
    "tag_map = {\n",
    "        'CC':None, # coordin. conjunction (and, but, or)  \n",
    "        'CD':wn.NOUN, # cardinal number (one, two)             \n",
    "        'DT':None, # determiner (a, the)                    \n",
    "        'EX':wn.ADV, # existential ‘there’ (there)           \n",
    "        'FW':None, # foreign word (mea culpa)             \n",
    "        'IN':wn.ADV, # preposition/sub-conj (of, in, by)   \n",
    "        'JJ':wn.ADJ, # adjective (yellow)                  \n",
    "        'JJR':wn.ADJ, # adj., comparative (bigger)          \n",
    "        'JJS':wn.ADJ, # adj., superlative (wildest)           \n",
    "        'LS':None, # list item marker (1, 2, One)          \n",
    "        'MD':None, # modal (can, should)                    \n",
    "        'NN':wn.NOUN, # noun, sing. or mass (llama)          \n",
    "        'NNS':wn.NOUN, # noun, plural (llamas)                  \n",
    "        'NNP':wn.NOUN, # proper noun, sing. (IBM)              \n",
    "        'NNPS':wn.NOUN, # proper noun, plural (Carolinas)\n",
    "        'PDT':wn.ADJ, # predeterminer (all, both)            \n",
    "        'POS':None, # possessive ending (’s )               \n",
    "        'PRP':None, # personal pronoun (I, you, he)     \n",
    "        'PRP$':None, # possessive pronoun (your, one’s)    \n",
    "        'RB':wn.ADV, # adverb (quickly, never)            \n",
    "        'RBR':wn.ADV, # adverb, comparative (faster)        \n",
    "        'RBS':wn.ADV, # adverb, superlative (fastest)     \n",
    "        'RP':wn.ADJ, # particle (up, off)\n",
    "        'SYM':None, # symbol (+,%, &)\n",
    "        'TO':None, # “to” (to)\n",
    "        'UH':None, # interjection (ah, oops)\n",
    "        'VB':wn.VERB, # verb base form (eat)\n",
    "        'VBD':wn.VERB, # verb past tense (ate)\n",
    "        'VBG':wn.VERB, # verb gerund (eating)\n",
    "        'VBN':wn.VERB, # verb past participle (eaten)\n",
    "        'VBP':wn.VERB, # verb non-3sg pres (eat)\n",
    "        'VBZ':wn.VERB, # verb 3sg pres (eats)\n",
    "        'WDT':None, # wh-determiner (which, that)\n",
    "        'WP':None, # wh-pronoun (what, who)\n",
    "        'WP$':None, # possessive (wh- whose)\n",
    "        'WRB':None, # wh-adverb (how, where)\n",
    "        '$':None, #  dollar sign ($)\n",
    "        '#':None, # pound sign (#)\n",
    "        '“':None, # left quote (‘ or “)\n",
    "        '”':None, # right quote (’ or ”)\n",
    "        '(':None, # left parenthesis ([, (, {, <)\n",
    "        ')':None, # right parenthesis (], ), }, >)\n",
    "        ',':None, # comma (,)\n",
    "        '.':None, # sentence-final punc (. ! ?)\n",
    "        ':':None # mid-sentence punc (: ; ... – -)\n",
    "    }\n",
    "\n",
    "def tokenize(text):     \n",
    "    ''' Tokenizer for CountVectorizer() \n",
    "\n",
    "        Inputs: \n",
    "            text: message instance\n",
    "        Output: \n",
    "            clean_tokens: list of lemmatized tokens based on words from the message\n",
    "    '''\n",
    "    #remove url links\n",
    "    re_url = r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?'\n",
    "    text = re.sub(re_url, 'urlplaceholder', text)\n",
    "\n",
    "    #remove punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())    \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # remove short words\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    \n",
    "    # remove stopwords\n",
    "    STOPWORDS = list(set(stopwords.words('english')))\n",
    "    tokens = [token for token in tokens if token not in STOPWORDS]\n",
    "\n",
    "    pos_tokens = pos_tag(tokens) \n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_tokens = []\n",
    "    for tok, pos in pos_tokens:\n",
    "        try:\n",
    "            if tag_map[pos] is not None:\n",
    "                clean_tok = lemmatizer.lemmatize(tok, tag_map[pos]).lower().strip()\n",
    "                clean_tokens.append(clean_tok)\n",
    "            else:\n",
    "                clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "                clean_tokens.append(clean_tok)\n",
    "        except KeyError:\n",
    "            pass\n",
    "            \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier(random_state = 42)))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenize at...\n",
       "                 MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                                        class_weight=None,\n",
       "                                                                        criterion='gini',\n",
       "                                                                        max_depth=None,\n",
       "                                                                        max_features='auto',\n",
       "                                                                        max_leaf_nodes=None,\n",
       "                                                                        min_impurity_decrease=0.0,\n",
       "                                                                        min_impurity_split=None,\n",
       "                                                                        min_samples_leaf=1,\n",
       "                                                                        min_samples_split=2,\n",
       "                                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                                        n_estimators='warn',\n",
       "                                                                        n_jobs=None,\n",
       "                                                                        oob_score=False,\n",
       "                                                                        random_state=42,\n",
       "                                                                        verbose=0,\n",
       "                                                                        warm_start=False),\n",
       "                                       n_jobs=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "pipeline.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 related\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.46      0.53      1266\n",
      "           1       0.84      0.91      0.87      3938\n",
      "           2       0.47      0.62      0.54        40\n",
      "\n",
      "    accuracy                           0.80      5244\n",
      "   macro avg       0.65      0.67      0.65      5244\n",
      "weighted avg       0.79      0.80      0.79      5244\n",
      "\n",
      "1 request\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93      4349\n",
      "           1       0.77      0.44      0.56       895\n",
      "\n",
      "    accuracy                           0.88      5244\n",
      "   macro avg       0.83      0.71      0.75      5244\n",
      "weighted avg       0.87      0.88      0.87      5244\n",
      "\n",
      "2 offer\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5218\n",
      "           1       0.00      0.00      0.00        26\n",
      "\n",
      "    accuracy                           1.00      5244\n",
      "   macro avg       0.50      0.50      0.50      5244\n",
      "weighted avg       0.99      1.00      0.99      5244\n",
      "\n",
      "3 aid_related\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.85      0.80      3113\n",
      "           1       0.73      0.61      0.67      2131\n",
      "\n",
      "    accuracy                           0.75      5244\n",
      "   macro avg       0.75      0.73      0.73      5244\n",
      "weighted avg       0.75      0.75      0.75      5244\n",
      "\n",
      "4 medical_help\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96      4822\n",
      "           1       0.52      0.10      0.17       422\n",
      "\n",
      "    accuracy                           0.92      5244\n",
      "   macro avg       0.73      0.55      0.56      5244\n",
      "weighted avg       0.89      0.92      0.89      5244\n",
      "\n",
      "5 medical_products\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98      4974\n",
      "           1       0.75      0.09      0.16       270\n",
      "\n",
      "    accuracy                           0.95      5244\n",
      "   macro avg       0.85      0.54      0.57      5244\n",
      "weighted avg       0.94      0.95      0.93      5244\n",
      "\n",
      "6 search_and_rescue\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      5117\n",
      "           1       0.50      0.03      0.06       127\n",
      "\n",
      "    accuracy                           0.98      5244\n",
      "   macro avg       0.74      0.52      0.52      5244\n",
      "weighted avg       0.96      0.98      0.97      5244\n",
      "\n",
      "7 security\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      5156\n",
      "           1       0.33      0.01      0.02        88\n",
      "\n",
      "    accuracy                           0.98      5244\n",
      "   macro avg       0.66      0.51      0.51      5244\n",
      "weighted avg       0.97      0.98      0.98      5244\n",
      "\n",
      "8 military\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99      5089\n",
      "           1       0.64      0.09      0.16       155\n",
      "\n",
      "    accuracy                           0.97      5244\n",
      "   macro avg       0.80      0.54      0.57      5244\n",
      "weighted avg       0.96      0.97      0.96      5244\n",
      "\n",
      "9 child_alone\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5244\n",
      "\n",
      "    accuracy                           1.00      5244\n",
      "   macro avg       1.00      1.00      1.00      5244\n",
      "weighted avg       1.00      1.00      1.00      5244\n",
      "\n",
      "10 water\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98      4905\n",
      "           1       0.81      0.36      0.50       339\n",
      "\n",
      "    accuracy                           0.95      5244\n",
      "   macro avg       0.89      0.68      0.74      5244\n",
      "weighted avg       0.95      0.95      0.94      5244\n",
      "\n",
      "11 food\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96      4649\n",
      "           1       0.85      0.42      0.56       595\n",
      "\n",
      "    accuracy                           0.93      5244\n",
      "   macro avg       0.89      0.70      0.76      5244\n",
      "weighted avg       0.92      0.93      0.91      5244\n",
      "\n",
      "12 shelter\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97      4774\n",
      "           1       0.81      0.37      0.51       470\n",
      "\n",
      "    accuracy                           0.94      5244\n",
      "   macro avg       0.88      0.68      0.74      5244\n",
      "weighted avg       0.93      0.94      0.93      5244\n",
      "\n",
      "13 clothing\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      5171\n",
      "           1       1.00      0.14      0.24        73\n",
      "\n",
      "    accuracy                           0.99      5244\n",
      "   macro avg       0.99      0.57      0.62      5244\n",
      "weighted avg       0.99      0.99      0.98      5244\n",
      "\n",
      "14 money\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      5140\n",
      "           1       0.62      0.08      0.14       104\n",
      "\n",
      "    accuracy                           0.98      5244\n",
      "   macro avg       0.80      0.54      0.56      5244\n",
      "weighted avg       0.97      0.98      0.97      5244\n",
      "\n",
      "15 missing_people\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      5184\n",
      "           1       1.00      0.02      0.03        60\n",
      "\n",
      "    accuracy                           0.99      5244\n",
      "   macro avg       0.99      0.51      0.51      5244\n",
      "weighted avg       0.99      0.99      0.98      5244\n",
      "\n",
      "16 refugees\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      5073\n",
      "           1       0.54      0.08      0.13       171\n",
      "\n",
      "    accuracy                           0.97      5244\n",
      "   macro avg       0.76      0.54      0.56      5244\n",
      "weighted avg       0.96      0.97      0.96      5244\n",
      "\n",
      "17 death\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      5007\n",
      "           1       0.79      0.17      0.28       237\n",
      "\n",
      "    accuracy                           0.96      5244\n",
      "   macro avg       0.88      0.59      0.63      5244\n",
      "weighted avg       0.95      0.96      0.95      5244\n",
      "\n",
      "18 other_aid\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.99      0.93      4549\n",
      "           1       0.51      0.04      0.08       695\n",
      "\n",
      "    accuracy                           0.87      5244\n",
      "   macro avg       0.69      0.52      0.50      5244\n",
      "weighted avg       0.82      0.87      0.82      5244\n",
      "\n",
      "19 infrastructure_related\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97      4916\n",
      "           1       0.25      0.01      0.02       328\n",
      "\n",
      "    accuracy                           0.94      5244\n",
      "   macro avg       0.59      0.50      0.49      5244\n",
      "weighted avg       0.89      0.94      0.91      5244\n",
      "\n",
      "20 transport\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      5004\n",
      "           1       0.61      0.07      0.13       240\n",
      "\n",
      "    accuracy                           0.96      5244\n",
      "   macro avg       0.78      0.53      0.55      5244\n",
      "weighted avg       0.94      0.96      0.94      5244\n",
      "\n",
      "21 buildings\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98      4977\n",
      "           1       0.77      0.11      0.20       267\n",
      "\n",
      "    accuracy                           0.95      5244\n",
      "   macro avg       0.86      0.56      0.59      5244\n",
      "weighted avg       0.95      0.95      0.94      5244\n",
      "\n",
      "22 electricity\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      5122\n",
      "           1       0.83      0.04      0.08       122\n",
      "\n",
      "    accuracy                           0.98      5244\n",
      "   macro avg       0.91      0.52      0.53      5244\n",
      "weighted avg       0.97      0.98      0.97      5244\n",
      "\n",
      "23 tools\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      5212\n",
      "           1       0.00      0.00      0.00        32\n",
      "\n",
      "    accuracy                           0.99      5244\n",
      "   macro avg       0.50      0.50      0.50      5244\n",
      "weighted avg       0.99      0.99      0.99      5244\n",
      "\n",
      "24 hospitals\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      5198\n",
      "           1       0.00      0.00      0.00        46\n",
      "\n",
      "    accuracy                           0.99      5244\n",
      "   macro avg       0.50      0.50      0.50      5244\n",
      "weighted avg       0.98      0.99      0.99      5244\n",
      "\n",
      "25 shops\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5222\n",
      "           1       0.00      0.00      0.00        22\n",
      "\n",
      "    accuracy                           1.00      5244\n",
      "   macro avg       0.50      0.50      0.50      5244\n",
      "weighted avg       0.99      1.00      0.99      5244\n",
      "\n",
      "26 aid_centers\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      5177\n",
      "           1       0.00      0.00      0.00        67\n",
      "\n",
      "    accuracy                           0.99      5244\n",
      "   macro avg       0.49      0.50      0.50      5244\n",
      "weighted avg       0.97      0.99      0.98      5244\n",
      "\n",
      "27 other_infrastructure\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      5021\n",
      "           1       0.00      0.00      0.00       223\n",
      "\n",
      "    accuracy                           0.96      5244\n",
      "   macro avg       0.48      0.50      0.49      5244\n",
      "weighted avg       0.92      0.96      0.94      5244\n",
      "\n",
      "28 weather_related\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.91      3806\n",
      "           1       0.84      0.60      0.70      1438\n",
      "\n",
      "    accuracy                           0.86      5244\n",
      "   macro avg       0.85      0.78      0.80      5244\n",
      "weighted avg       0.86      0.86      0.85      5244\n",
      "\n",
      "29 floods\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      4833\n",
      "           1       0.87      0.36      0.51       411\n",
      "\n",
      "    accuracy                           0.95      5244\n",
      "   macro avg       0.91      0.68      0.74      5244\n",
      "weighted avg       0.94      0.95      0.93      5244\n",
      "\n",
      "30 storm\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96      4758\n",
      "           1       0.74      0.40      0.52       486\n",
      "\n",
      "    accuracy                           0.93      5244\n",
      "   macro avg       0.84      0.69      0.74      5244\n",
      "weighted avg       0.92      0.93      0.92      5244\n",
      "\n",
      "31 fire\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      5191\n",
      "           1       0.50      0.02      0.04        53\n",
      "\n",
      "    accuracy                           0.99      5244\n",
      "   macro avg       0.75      0.51      0.52      5244\n",
      "weighted avg       0.99      0.99      0.99      5244\n",
      "\n",
      "32 earthquake\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      4766\n",
      "           1       0.86      0.68      0.76       478\n",
      "\n",
      "    accuracy                           0.96      5244\n",
      "   macro avg       0.92      0.84      0.87      5244\n",
      "weighted avg       0.96      0.96      0.96      5244\n",
      "\n",
      "33 cold\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      5127\n",
      "           1       0.85      0.09      0.17       117\n",
      "\n",
      "    accuracy                           0.98      5244\n",
      "   macro avg       0.91      0.55      0.58      5244\n",
      "weighted avg       0.98      0.98      0.97      5244\n",
      "\n",
      "34 other_weather\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      4968\n",
      "           1       0.44      0.03      0.05       276\n",
      "\n",
      "    accuracy                           0.95      5244\n",
      "   macro avg       0.69      0.51      0.51      5244\n",
      "weighted avg       0.92      0.95      0.92      5244\n",
      "\n",
      "35 direct_report\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.91      4223\n",
      "           1       0.69      0.32      0.44      1021\n",
      "\n",
      "    accuracy                           0.84      5244\n",
      "   macro avg       0.77      0.64      0.67      5244\n",
      "weighted avg       0.82      0.84      0.82      5244\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kbosko/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "for i, col in enumerate(Y.columns):\n",
    "    print(i, col)\n",
    "    print(classification_report(Y_test.to_numpy()[:, i], Y_pred[:, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'related': 1,\n",
       " 'request': 1,\n",
       " 'offer': 0,\n",
       " 'aid_related': 1,\n",
       " 'medical_help': 0,\n",
       " 'medical_products': 0,\n",
       " 'search_and_rescue': 0,\n",
       " 'security': 0,\n",
       " 'military': 0,\n",
       " 'child_alone': 0,\n",
       " 'water': 0,\n",
       " 'food': 1,\n",
       " 'shelter': 1,\n",
       " 'clothing': 0,\n",
       " 'money': 0,\n",
       " 'missing_people': 0,\n",
       " 'refugees': 0,\n",
       " 'death': 0,\n",
       " 'other_aid': 0,\n",
       " 'infrastructure_related': 0,\n",
       " 'transport': 0,\n",
       " 'buildings': 0,\n",
       " 'electricity': 0,\n",
       " 'tools': 0,\n",
       " 'hospitals': 0,\n",
       " 'shops': 0,\n",
       " 'aid_centers': 0,\n",
       " 'other_infrastructure': 0,\n",
       " 'weather_related': 0,\n",
       " 'floods': 0,\n",
       " 'storm': 0,\n",
       " 'fire': 0,\n",
       " 'earthquake': 0,\n",
       " 'cold': 0,\n",
       " 'other_weather': 0,\n",
       " 'direct_report': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_labels = pipeline.predict(['We are more than 50 people sleeping on the street. Please, help us find tent, food.'])[0]\n",
    "classification_results = dict(zip(df.columns[4:], classification_labels))\n",
    "classification_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'vect', 'tfidf', 'clf', 'vect__analyzer', 'vect__binary', 'vect__decode_error', 'vect__dtype', 'vect__encoding', 'vect__input', 'vect__lowercase', 'vect__max_df', 'vect__max_features', 'vect__min_df', 'vect__ngram_range', 'vect__preprocessor', 'vect__stop_words', 'vect__strip_accents', 'vect__token_pattern', 'vect__tokenizer', 'vect__vocabulary', 'tfidf__norm', 'tfidf__smooth_idf', 'tfidf__sublinear_tf', 'tfidf__use_idf', 'clf__estimator__bootstrap', 'clf__estimator__class_weight', 'clf__estimator__criterion', 'clf__estimator__max_depth', 'clf__estimator__max_features', 'clf__estimator__max_leaf_nodes', 'clf__estimator__min_impurity_decrease', 'clf__estimator__min_impurity_split', 'clf__estimator__min_samples_leaf', 'clf__estimator__min_samples_split', 'clf__estimator__min_weight_fraction_leaf', 'clf__estimator__n_estimators', 'clf__estimator__n_jobs', 'clf__estimator__oob_score', 'clf__estimator__random_state', 'clf__estimator__verbose', 'clf__estimator__warm_start', 'clf__estimator', 'clf__n_jobs'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords', quiet=True, raise_on_error=True)\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "tokenized_stop_words = nltk.word_tokenize(' '.join(nltk.corpus.stopwords.words('english')))\n",
    "tokenized_stop_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'vect__stop_words': (tokenized_stop_words, None),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "    'vect__max_df': (0.5, 0.75, 1.0)\n",
    "    }\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters, verbose=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[CV] vect__max_df=0.5, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  vect__max_df=0.5, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"], score=0.238, total=  41.6s\n",
      "[CV] vect__max_df=0.5, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   41.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  vect__max_df=0.5, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"], score=0.232, total=  44.6s\n",
      "[CV] vect__max_df=0.5, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  vect__max_df=0.5, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"], score=0.245, total=  54.8s\n",
      "[CV] vect__max_df=0.5, vect__ngram_range=(1, 1), vect__stop_words=None \n",
      "[CV]  vect__max_df=0.5, vect__ngram_range=(1, 1), vect__stop_words=None, score=0.240, total=  44.4s\n",
      "[CV] vect__max_df=0.5, vect__ngram_range=(1, 1), vect__stop_words=None \n",
      "[CV]  vect__max_df=0.5, vect__ngram_range=(1, 1), vect__stop_words=None, score=0.231, total=  42.7s\n",
      "[CV] vect__max_df=0.5, vect__ngram_range=(1, 1), vect__stop_words=None \n",
      "[CV]  vect__max_df=0.5, vect__ngram_range=(1, 1), vect__stop_words=None, score=0.247, total=  42.6s\n",
      "[CV] vect__max_df=0.5, vect__ngram_range=(1, 2), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"] \n",
      "[CV]  vect__max_df=0.5, vect__ngram_range=(1, 2), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"], score=0.242, total= 1.2min\n",
      "[CV] vect__max_df=0.5, vect__ngram_range=(1, 2), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"] \n",
      "[CV]  vect__max_df=0.5, vect__ngram_range=(1, 2), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"], score=0.239, total= 1.3min\n",
      "[CV] vect__max_df=0.5, vect__ngram_range=(1, 2), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  vect__max_df=0.5, vect__ngram_range=(1, 2), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"], score=0.247, total= 1.1min\n",
      "[CV] vect__max_df=0.5, vect__ngram_range=(1, 2), vect__stop_words=None \n",
      "[CV]  vect__max_df=0.5, vect__ngram_range=(1, 2), vect__stop_words=None, score=0.232, total= 1.0min\n",
      "[CV] vect__max_df=0.5, vect__ngram_range=(1, 2), vect__stop_words=None \n",
      "[CV]  vect__max_df=0.5, vect__ngram_range=(1, 2), vect__stop_words=None, score=0.238, total=  59.7s\n",
      "[CV] vect__max_df=0.5, vect__ngram_range=(1, 2), vect__stop_words=None \n",
      "[CV]  vect__max_df=0.5, vect__ngram_range=(1, 2), vect__stop_words=None, score=0.248, total= 1.1min\n",
      "[CV] vect__max_df=0.75, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"] \n",
      "[CV]  vect__max_df=0.75, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"], score=0.238, total=  47.8s\n",
      "[CV] vect__max_df=0.75, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"] \n",
      "[CV]  vect__max_df=0.75, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"], score=0.232, total=  45.9s\n",
      "[CV] vect__max_df=0.75, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  vect__max_df=0.75, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"], score=0.245, total=  43.8s\n",
      "[CV] vect__max_df=0.75, vect__ngram_range=(1, 1), vect__stop_words=None \n",
      "[CV]  vect__max_df=0.75, vect__ngram_range=(1, 1), vect__stop_words=None, score=0.240, total=  42.1s\n",
      "[CV] vect__max_df=0.75, vect__ngram_range=(1, 1), vect__stop_words=None \n",
      "[CV]  vect__max_df=0.75, vect__ngram_range=(1, 1), vect__stop_words=None, score=0.231, total=  42.5s\n",
      "[CV] vect__max_df=0.75, vect__ngram_range=(1, 1), vect__stop_words=None \n",
      "[CV]  vect__max_df=0.75, vect__ngram_range=(1, 1), vect__stop_words=None, score=0.247, total=  42.4s\n",
      "[CV] vect__max_df=0.75, vect__ngram_range=(1, 2), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"] \n",
      "[CV]  vect__max_df=0.75, vect__ngram_range=(1, 2), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"], score=0.242, total= 1.2min\n",
      "[CV] vect__max_df=0.75, vect__ngram_range=(1, 2), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"] \n",
      "[CV]  vect__max_df=0.75, vect__ngram_range=(1, 2), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"], score=0.239, total= 1.1min\n",
      "[CV] vect__max_df=0.75, vect__ngram_range=(1, 2), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  vect__max_df=0.75, vect__ngram_range=(1, 2), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"], score=0.247, total= 1.1min\n",
      "[CV] vect__max_df=0.75, vect__ngram_range=(1, 2), vect__stop_words=None \n",
      "[CV]  vect__max_df=0.75, vect__ngram_range=(1, 2), vect__stop_words=None, score=0.232, total= 1.1min\n",
      "[CV] vect__max_df=0.75, vect__ngram_range=(1, 2), vect__stop_words=None \n",
      "[CV]  vect__max_df=0.75, vect__ngram_range=(1, 2), vect__stop_words=None, score=0.238, total= 1.0min\n",
      "[CV] vect__max_df=0.75, vect__ngram_range=(1, 2), vect__stop_words=None \n",
      "[CV]  vect__max_df=0.75, vect__ngram_range=(1, 2), vect__stop_words=None, score=0.248, total= 1.1min\n",
      "[CV] vect__max_df=1.0, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"] \n",
      "[CV]  vect__max_df=1.0, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"], score=0.238, total=  43.8s\n",
      "[CV] vect__max_df=1.0, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"] \n",
      "[CV]  vect__max_df=1.0, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"], score=0.232, total=  42.5s\n",
      "[CV] vect__max_df=1.0, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  vect__max_df=1.0, vect__ngram_range=(1, 1), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"], score=0.245, total=  43.0s\n",
      "[CV] vect__max_df=1.0, vect__ngram_range=(1, 1), vect__stop_words=None \n",
      "[CV]  vect__max_df=1.0, vect__ngram_range=(1, 1), vect__stop_words=None, score=0.240, total=  43.4s\n",
      "[CV] vect__max_df=1.0, vect__ngram_range=(1, 1), vect__stop_words=None \n",
      "[CV]  vect__max_df=1.0, vect__ngram_range=(1, 1), vect__stop_words=None, score=0.231, total=  42.8s\n",
      "[CV] vect__max_df=1.0, vect__ngram_range=(1, 1), vect__stop_words=None \n",
      "[CV]  vect__max_df=1.0, vect__ngram_range=(1, 1), vect__stop_words=None, score=0.247, total=  42.7s\n",
      "[CV] vect__max_df=1.0, vect__ngram_range=(1, 2), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"] \n",
      "[CV]  vect__max_df=1.0, vect__ngram_range=(1, 2), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"], score=0.242, total= 1.0min\n",
      "[CV] vect__max_df=1.0, vect__ngram_range=(1, 2), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"] \n",
      "[CV]  vect__max_df=1.0, vect__ngram_range=(1, 2), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"], score=0.239, total= 1.0min\n",
      "[CV] vect__max_df=1.0, vect__ngram_range=(1, 2), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  vect__max_df=1.0, vect__ngram_range=(1, 2), vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"], score=0.247, total= 1.1min\n",
      "[CV] vect__max_df=1.0, vect__ngram_range=(1, 2), vect__stop_words=None \n",
      "[CV]  vect__max_df=1.0, vect__ngram_range=(1, 2), vect__stop_words=None, score=0.232, total= 1.1min\n",
      "[CV] vect__max_df=1.0, vect__ngram_range=(1, 2), vect__stop_words=None \n",
      "[CV]  vect__max_df=1.0, vect__ngram_range=(1, 2), vect__stop_words=None, score=0.238, total= 1.3min\n",
      "[CV] vect__max_df=1.0, vect__ngram_range=(1, 2), vect__stop_words=None \n",
      "[CV]  vect__max_df=1.0, vect__ngram_range=(1, 2), vect__stop_words=None, score=0.248, total= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed: 33.3min finished\n"
     ]
    }
   ],
   "source": [
    "cv.fit(X_train, Y_train)\n",
    "Y_pred = cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vect__max_df': 0.5, 'vect__ngram_range': (1, 2), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'you', \"'re\", 'you', \"'ve\", 'you', \"'ll\", 'you', \"'d\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'she', \"'s\", 'her', 'hers', 'herself', 'it', 'it', \"'s\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'that', \"'ll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do', \"n't\", 'should', 'should', \"'ve\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'are', \"n't\", 'couldn', 'could', \"n't\", 'didn', 'did', \"n't\", 'doesn', 'does', \"n't\", 'hadn', 'had', \"n't\", 'hasn', 'has', \"n't\", 'haven', 'have', \"n't\", 'isn', 'is', \"n't\", 'ma', 'mightn', 'might', \"n't\", 'mustn', 'must', \"n't\", 'needn', 'need', \"n't\", 'shan', 'sha', \"n't\", 'shouldn', 'should', \"n't\", 'wasn', 'was', \"n't\", 'weren', 'were', \"n't\", 'won', 'wo', \"n't\", 'wouldn', 'would', \"n't\"]} 0.24246614533663932\n"
     ]
    }
   ],
   "source": [
    "print(cv.best_params_, cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 related\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.50      0.55      1266\n",
      "           1       0.85      0.88      0.87      3938\n",
      "           2       0.39      0.55      0.45        40\n",
      "\n",
      "    accuracy                           0.79      5244\n",
      "   macro avg       0.61      0.65      0.62      5244\n",
      "weighted avg       0.78      0.79      0.78      5244\n",
      "\n",
      "1 request\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93      4349\n",
      "           1       0.76      0.40      0.52       895\n",
      "\n",
      "    accuracy                           0.88      5244\n",
      "   macro avg       0.82      0.69      0.73      5244\n",
      "weighted avg       0.87      0.88      0.86      5244\n",
      "\n",
      "2 offer\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5218\n",
      "           1       0.00      0.00      0.00        26\n",
      "\n",
      "    accuracy                           1.00      5244\n",
      "   macro avg       0.50      0.50      0.50      5244\n",
      "weighted avg       0.99      1.00      0.99      5244\n",
      "\n",
      "3 aid_related\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.88      0.80      3113\n",
      "           1       0.76      0.54      0.63      2131\n",
      "\n",
      "    accuracy                           0.74      5244\n",
      "   macro avg       0.75      0.71      0.72      5244\n",
      "weighted avg       0.74      0.74      0.73      5244\n",
      "\n",
      "4 medical_help\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96      4822\n",
      "           1       0.56      0.07      0.12       422\n",
      "\n",
      "    accuracy                           0.92      5244\n",
      "   macro avg       0.74      0.53      0.54      5244\n",
      "weighted avg       0.89      0.92      0.89      5244\n",
      "\n",
      "5 medical_products\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98      4974\n",
      "           1       0.91      0.12      0.21       270\n",
      "\n",
      "    accuracy                           0.95      5244\n",
      "   macro avg       0.93      0.56      0.59      5244\n",
      "weighted avg       0.95      0.95      0.94      5244\n",
      "\n",
      "6 search_and_rescue\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      5117\n",
      "           1       0.86      0.09      0.17       127\n",
      "\n",
      "    accuracy                           0.98      5244\n",
      "   macro avg       0.92      0.55      0.58      5244\n",
      "weighted avg       0.98      0.98      0.97      5244\n",
      "\n",
      "7 security\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      5156\n",
      "           1       0.00      0.00      0.00        88\n",
      "\n",
      "    accuracy                           0.98      5244\n",
      "   macro avg       0.49      0.50      0.50      5244\n",
      "weighted avg       0.97      0.98      0.97      5244\n",
      "\n",
      "8 military\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99      5089\n",
      "           1       0.62      0.08      0.15       155\n",
      "\n",
      "    accuracy                           0.97      5244\n",
      "   macro avg       0.80      0.54      0.57      5244\n",
      "weighted avg       0.96      0.97      0.96      5244\n",
      "\n",
      "9 child_alone\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5244\n",
      "\n",
      "    accuracy                           1.00      5244\n",
      "   macro avg       1.00      1.00      1.00      5244\n",
      "weighted avg       1.00      1.00      1.00      5244\n",
      "\n",
      "10 water\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98      4905\n",
      "           1       0.80      0.42      0.55       339\n",
      "\n",
      "    accuracy                           0.96      5244\n",
      "   macro avg       0.88      0.71      0.77      5244\n",
      "weighted avg       0.95      0.96      0.95      5244\n",
      "\n",
      "11 food\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96      4649\n",
      "           1       0.85      0.44      0.58       595\n",
      "\n",
      "    accuracy                           0.93      5244\n",
      "   macro avg       0.89      0.72      0.77      5244\n",
      "weighted avg       0.92      0.93      0.92      5244\n",
      "\n",
      "12 shelter\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97      4774\n",
      "           1       0.83      0.36      0.50       470\n",
      "\n",
      "    accuracy                           0.94      5244\n",
      "   macro avg       0.88      0.68      0.73      5244\n",
      "weighted avg       0.93      0.94      0.92      5244\n",
      "\n",
      "13 clothing\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      5171\n",
      "           1       0.90      0.12      0.22        73\n",
      "\n",
      "    accuracy                           0.99      5244\n",
      "   macro avg       0.94      0.56      0.61      5244\n",
      "weighted avg       0.99      0.99      0.98      5244\n",
      "\n",
      "14 money\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      5140\n",
      "           1       0.89      0.08      0.14       104\n",
      "\n",
      "    accuracy                           0.98      5244\n",
      "   macro avg       0.94      0.54      0.57      5244\n",
      "weighted avg       0.98      0.98      0.97      5244\n",
      "\n",
      "15 missing_people\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      5184\n",
      "           1       0.50      0.03      0.06        60\n",
      "\n",
      "    accuracy                           0.99      5244\n",
      "   macro avg       0.74      0.52      0.53      5244\n",
      "weighted avg       0.98      0.99      0.98      5244\n",
      "\n",
      "16 refugees\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      5073\n",
      "           1       0.69      0.05      0.10       171\n",
      "\n",
      "    accuracy                           0.97      5244\n",
      "   macro avg       0.83      0.53      0.54      5244\n",
      "weighted avg       0.96      0.97      0.95      5244\n",
      "\n",
      "17 death\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      5007\n",
      "           1       0.81      0.16      0.27       237\n",
      "\n",
      "    accuracy                           0.96      5244\n",
      "   macro avg       0.89      0.58      0.62      5244\n",
      "weighted avg       0.95      0.96      0.95      5244\n",
      "\n",
      "18 other_aid\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.99      0.93      4549\n",
      "           1       0.47      0.05      0.09       695\n",
      "\n",
      "    accuracy                           0.87      5244\n",
      "   macro avg       0.67      0.52      0.51      5244\n",
      "weighted avg       0.82      0.87      0.82      5244\n",
      "\n",
      "19 infrastructure_related\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97      4916\n",
      "           1       0.40      0.01      0.01       328\n",
      "\n",
      "    accuracy                           0.94      5244\n",
      "   macro avg       0.67      0.50      0.49      5244\n",
      "weighted avg       0.90      0.94      0.91      5244\n",
      "\n",
      "20 transport\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      5004\n",
      "           1       0.79      0.06      0.12       240\n",
      "\n",
      "    accuracy                           0.96      5244\n",
      "   macro avg       0.87      0.53      0.55      5244\n",
      "weighted avg       0.95      0.96      0.94      5244\n",
      "\n",
      "21 buildings\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      4977\n",
      "           1       0.77      0.13      0.23       267\n",
      "\n",
      "    accuracy                           0.95      5244\n",
      "   macro avg       0.86      0.57      0.60      5244\n",
      "weighted avg       0.95      0.95      0.94      5244\n",
      "\n",
      "22 electricity\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      5122\n",
      "           1       1.00      0.02      0.05       122\n",
      "\n",
      "    accuracy                           0.98      5244\n",
      "   macro avg       0.99      0.51      0.52      5244\n",
      "weighted avg       0.98      0.98      0.97      5244\n",
      "\n",
      "23 tools\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      5212\n",
      "           1       1.00      0.03      0.06        32\n",
      "\n",
      "    accuracy                           0.99      5244\n",
      "   macro avg       1.00      0.52      0.53      5244\n",
      "weighted avg       0.99      0.99      0.99      5244\n",
      "\n",
      "24 hospitals\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      5198\n",
      "           1       0.00      0.00      0.00        46\n",
      "\n",
      "    accuracy                           0.99      5244\n",
      "   macro avg       0.50      0.50      0.50      5244\n",
      "weighted avg       0.98      0.99      0.99      5244\n",
      "\n",
      "25 shops\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5222\n",
      "           1       0.00      0.00      0.00        22\n",
      "\n",
      "    accuracy                           1.00      5244\n",
      "   macro avg       0.50      0.50      0.50      5244\n",
      "weighted avg       0.99      1.00      0.99      5244\n",
      "\n",
      "26 aid_centers\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      5177\n",
      "           1       0.00      0.00      0.00        67\n",
      "\n",
      "    accuracy                           0.99      5244\n",
      "   macro avg       0.49      0.50      0.50      5244\n",
      "weighted avg       0.97      0.99      0.98      5244\n",
      "\n",
      "27 other_infrastructure\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      5021\n",
      "           1       0.00      0.00      0.00       223\n",
      "\n",
      "    accuracy                           0.96      5244\n",
      "   macro avg       0.48      0.50      0.49      5244\n",
      "weighted avg       0.92      0.96      0.94      5244\n",
      "\n",
      "28 weather_related\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.96      0.90      3806\n",
      "           1       0.84      0.50      0.63      1438\n",
      "\n",
      "    accuracy                           0.84      5244\n",
      "   macro avg       0.84      0.73      0.76      5244\n",
      "weighted avg       0.84      0.84      0.82      5244\n",
      "\n",
      "29 floods\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      4833\n",
      "           1       0.86      0.32      0.47       411\n",
      "\n",
      "    accuracy                           0.94      5244\n",
      "   macro avg       0.90      0.66      0.72      5244\n",
      "weighted avg       0.94      0.94      0.93      5244\n",
      "\n",
      "30 storm\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96      4758\n",
      "           1       0.75      0.33      0.46       486\n",
      "\n",
      "    accuracy                           0.93      5244\n",
      "   macro avg       0.84      0.66      0.71      5244\n",
      "weighted avg       0.92      0.93      0.91      5244\n",
      "\n",
      "31 fire\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      5191\n",
      "           1       1.00      0.04      0.07        53\n",
      "\n",
      "    accuracy                           0.99      5244\n",
      "   macro avg       1.00      0.52      0.53      5244\n",
      "weighted avg       0.99      0.99      0.99      5244\n",
      "\n",
      "32 earthquake\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      4766\n",
      "           1       0.87      0.70      0.77       478\n",
      "\n",
      "    accuracy                           0.96      5244\n",
      "   macro avg       0.92      0.84      0.88      5244\n",
      "weighted avg       0.96      0.96      0.96      5244\n",
      "\n",
      "33 cold\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      5127\n",
      "           1       0.75      0.05      0.10       117\n",
      "\n",
      "    accuracy                           0.98      5244\n",
      "   macro avg       0.86      0.53      0.54      5244\n",
      "weighted avg       0.97      0.98      0.97      5244\n",
      "\n",
      "34 other_weather\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      4968\n",
      "           1       0.55      0.02      0.04       276\n",
      "\n",
      "    accuracy                           0.95      5244\n",
      "   macro avg       0.75      0.51      0.51      5244\n",
      "weighted avg       0.93      0.95      0.92      5244\n",
      "\n",
      "35 direct_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.97      0.90      4223\n",
      "           1       0.66      0.27      0.39      1021\n",
      "\n",
      "    accuracy                           0.83      5244\n",
      "   macro avg       0.76      0.62      0.64      5244\n",
      "weighted avg       0.81      0.83      0.80      5244\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kbosko/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "for i, col in enumerate(Y.columns):\n",
    "    print(i, col)\n",
    "    print(classification_report(Y_test.to_numpy()[:, i], Y_pred[:, i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifying the best parameters that were determined in the previous gridsearch step\n",
    "pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize, max_df=0.5, ngram_range=(1, 2), stop_words=tokenized_stop_words)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier(random_state = 42)))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying out different ML algrorithms \n",
    "parameters = {\n",
    "    'clf': (MultiOutputClassifier(SGDClassifier(random_state = 42)),\n",
    "            MultiOutputClassifier(RandomForestClassifier(random_state = 42)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = GridSearchCV(pipeline, param_grid=parameters, verbose=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that we can't run SGDClassifier on the original dataset. The reason is that there's a column \"child_alone\" that has no positive instances! That's why I'm dropping here this category and do the train-test split again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nochildalone = df.drop(['child_alone'], axis=1)\n",
    "\n",
    "X_nochildalone = df_nochildalone['message']\n",
    "Y_nochildalone = df_nochildalone.iloc[:, 4:]\n",
    "X_train_nca, X_test_nca, Y_train_nca, Y_test_nca = train_test_split(X_nochildalone, Y_nochildalone, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "[CV] clf=MultiOutputClassifier(estimator=SGDClassifier(alpha=0.0001, average=False,\n",
      "                                              class_weight=None,\n",
      "                                              early_stopping=False, epsilon=0.1,\n",
      "                                              eta0=0.0, fit_intercept=True,\n",
      "                                              l1_ratio=0.15,\n",
      "                                              learning_rate='optimal',\n",
      "                                              loss='hinge', max_iter=1000,\n",
      "                                              n_iter_no_change=5, n_jobs=None,\n",
      "                                              penalty='l2', power_t=0.5,\n",
      "                                              random_state=42, shuffle=True,\n",
      "                                              tol=0.001,\n",
      "                                              validation_fraction=0.1,\n",
      "                                              verbose=0, warm_start=False),\n",
      "                      n_jobs=None) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultiOutputClassifier(estimator=SGDClassifier(alpha=0.0001, average=False,\n",
      "                                              class_weight=None,\n",
      "                                              early_stopping=False, epsilon=0.1,\n",
      "                                              eta0=0.0, fit_intercept=True,\n",
      "                                              l1_ratio=0.15,\n",
      "                                              learning_rate='optimal',\n",
      "                                              loss='hinge', max_iter=1000,\n",
      "                                              n_iter_no_change=5, n_jobs=None,\n",
      "                                              penalty='l2', power_t=0.5,\n",
      "                                              random_state=42, shuffle=True,\n",
      "                                              tol=0.001,\n",
      "                                              validation_fraction=0.1,\n",
      "                                              verbose=0, warm_start=False),\n",
      "                      n_jobs=None), score=0.269, total=  25.7s\n",
      "[CV] clf=MultiOutputClassifier(estimator=SGDClassifier(alpha=0.0001, average=False,\n",
      "                                              class_weight=None,\n",
      "                                              early_stopping=False, epsilon=0.1,\n",
      "                                              eta0=0.0, fit_intercept=True,\n",
      "                                              l1_ratio=0.15,\n",
      "                                              learning_rate='optimal',\n",
      "                                              loss='hinge', max_iter=1000,\n",
      "                                              n_iter_no_change=5, n_jobs=None,\n",
      "                                              penalty='l2', power_t=0.5,\n",
      "                                              random_state=42, shuffle=True,\n",
      "                                              tol=0.001,\n",
      "                                              validation_fraction=0.1,\n",
      "                                              verbose=0, warm_start=False),\n",
      "                      n_jobs=None) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   25.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultiOutputClassifier(estimator=SGDClassifier(alpha=0.0001, average=False,\n",
      "                                              class_weight=None,\n",
      "                                              early_stopping=False, epsilon=0.1,\n",
      "                                              eta0=0.0, fit_intercept=True,\n",
      "                                              l1_ratio=0.15,\n",
      "                                              learning_rate='optimal',\n",
      "                                              loss='hinge', max_iter=1000,\n",
      "                                              n_iter_no_change=5, n_jobs=None,\n",
      "                                              penalty='l2', power_t=0.5,\n",
      "                                              random_state=42, shuffle=True,\n",
      "                                              tol=0.001,\n",
      "                                              validation_fraction=0.1,\n",
      "                                              verbose=0, warm_start=False),\n",
      "                      n_jobs=None), score=0.275, total=  25.5s\n",
      "[CV] clf=MultiOutputClassifier(estimator=SGDClassifier(alpha=0.0001, average=False,\n",
      "                                              class_weight=None,\n",
      "                                              early_stopping=False, epsilon=0.1,\n",
      "                                              eta0=0.0, fit_intercept=True,\n",
      "                                              l1_ratio=0.15,\n",
      "                                              learning_rate='optimal',\n",
      "                                              loss='hinge', max_iter=1000,\n",
      "                                              n_iter_no_change=5, n_jobs=None,\n",
      "                                              penalty='l2', power_t=0.5,\n",
      "                                              random_state=42, shuffle=True,\n",
      "                                              tol=0.001,\n",
      "                                              validation_fraction=0.1,\n",
      "                                              verbose=0, warm_start=False),\n",
      "                      n_jobs=None) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   51.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultiOutputClassifier(estimator=SGDClassifier(alpha=0.0001, average=False,\n",
      "                                              class_weight=None,\n",
      "                                              early_stopping=False, epsilon=0.1,\n",
      "                                              eta0=0.0, fit_intercept=True,\n",
      "                                              l1_ratio=0.15,\n",
      "                                              learning_rate='optimal',\n",
      "                                              loss='hinge', max_iter=1000,\n",
      "                                              n_iter_no_change=5, n_jobs=None,\n",
      "                                              penalty='l2', power_t=0.5,\n",
      "                                              random_state=42, shuffle=True,\n",
      "                                              tol=0.001,\n",
      "                                              validation_fraction=0.1,\n",
      "                                              verbose=0, warm_start=False),\n",
      "                      n_jobs=None), score=0.279, total=  26.3s\n",
      "[CV] clf=MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
      "                                                       class_weight=None,\n",
      "                                                       criterion='gini',\n",
      "                                                       max_depth=None,\n",
      "                                                       max_features='auto',\n",
      "                                                       max_leaf_nodes=None,\n",
      "                                                       min_impurity_decrease=0.0,\n",
      "                                                       min_impurity_split=None,\n",
      "                                                       min_samples_leaf=1,\n",
      "                                                       min_samples_split=2,\n",
      "                                                       min_weight_fraction_leaf=0.0,\n",
      "                                                       n_estimators='warn',\n",
      "                                                       n_jobs=None,\n",
      "                                                       oob_score=False,\n",
      "                                                       random_state=42,\n",
      "                                                       verbose=0,\n",
      "                                                       warm_start=False),\n",
      "                      n_jobs=None) \n",
      "[CV]  clf=MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
      "                                                       class_weight=None,\n",
      "                                                       criterion='gini',\n",
      "                                                       max_depth=None,\n",
      "                                                       max_features='auto',\n",
      "                                                       max_leaf_nodes=None,\n",
      "                                                       min_impurity_decrease=0.0,\n",
      "                                                       min_impurity_split=None,\n",
      "                                                       min_samples_leaf=1,\n",
      "                                                       min_samples_split=2,\n",
      "                                                       min_weight_fraction_leaf=0.0,\n",
      "                                                       n_estimators='warn',\n",
      "                                                       n_jobs=None,\n",
      "                                                       oob_score=False,\n",
      "                                                       random_state=42,\n",
      "                                                       verbose=0,\n",
      "                                                       warm_start=False),\n",
      "                      n_jobs=None), score=0.242, total= 1.1min\n",
      "[CV] clf=MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
      "                                                       class_weight=None,\n",
      "                                                       criterion='gini',\n",
      "                                                       max_depth=None,\n",
      "                                                       max_features='auto',\n",
      "                                                       max_leaf_nodes=None,\n",
      "                                                       min_impurity_decrease=0.0,\n",
      "                                                       min_impurity_split=None,\n",
      "                                                       min_samples_leaf=1,\n",
      "                                                       min_samples_split=2,\n",
      "                                                       min_weight_fraction_leaf=0.0,\n",
      "                                                       n_estimators='warn',\n",
      "                                                       n_jobs=None,\n",
      "                                                       oob_score=False,\n",
      "                                                       random_state=42,\n",
      "                                                       verbose=0,\n",
      "                                                       warm_start=False),\n",
      "                      n_jobs=None) \n",
      "[CV]  clf=MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
      "                                                       class_weight=None,\n",
      "                                                       criterion='gini',\n",
      "                                                       max_depth=None,\n",
      "                                                       max_features='auto',\n",
      "                                                       max_leaf_nodes=None,\n",
      "                                                       min_impurity_decrease=0.0,\n",
      "                                                       min_impurity_split=None,\n",
      "                                                       min_samples_leaf=1,\n",
      "                                                       min_samples_split=2,\n",
      "                                                       min_weight_fraction_leaf=0.0,\n",
      "                                                       n_estimators='warn',\n",
      "                                                       n_jobs=None,\n",
      "                                                       oob_score=False,\n",
      "                                                       random_state=42,\n",
      "                                                       verbose=0,\n",
      "                                                       warm_start=False),\n",
      "                      n_jobs=None), score=0.239, total= 1.2min\n",
      "[CV] clf=MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
      "                                                       class_weight=None,\n",
      "                                                       criterion='gini',\n",
      "                                                       max_depth=None,\n",
      "                                                       max_features='auto',\n",
      "                                                       max_leaf_nodes=None,\n",
      "                                                       min_impurity_decrease=0.0,\n",
      "                                                       min_impurity_split=None,\n",
      "                                                       min_samples_leaf=1,\n",
      "                                                       min_samples_split=2,\n",
      "                                                       min_weight_fraction_leaf=0.0,\n",
      "                                                       n_estimators='warn',\n",
      "                                                       n_jobs=None,\n",
      "                                                       oob_score=False,\n",
      "                                                       random_state=42,\n",
      "                                                       verbose=0,\n",
      "                                                       warm_start=False),\n",
      "                      n_jobs=None) \n",
      "[CV]  clf=MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
      "                                                       class_weight=None,\n",
      "                                                       criterion='gini',\n",
      "                                                       max_depth=None,\n",
      "                                                       max_features='auto',\n",
      "                                                       max_leaf_nodes=None,\n",
      "                                                       min_impurity_decrease=0.0,\n",
      "                                                       min_impurity_split=None,\n",
      "                                                       min_samples_leaf=1,\n",
      "                                                       min_samples_split=2,\n",
      "                                                       min_weight_fraction_leaf=0.0,\n",
      "                                                       n_estimators='warn',\n",
      "                                                       n_jobs=None,\n",
      "                                                       oob_score=False,\n",
      "                                                       random_state=42,\n",
      "                                                       verbose=0,\n",
      "                                                       warm_start=False),\n",
      "                      n_jobs=None), score=0.247, total= 1.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  4.6min finished\n"
     ]
    }
   ],
   "source": [
    "cv.fit(X_train_nca, Y_train_nca)\n",
    "Y_pred = cv.predict(X_test_nca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf': MultiOutputClassifier(estimator=SGDClassifier(alpha=0.0001, average=False,\n",
      "                                              class_weight=None,\n",
      "                                              early_stopping=False, epsilon=0.1,\n",
      "                                              eta0=0.0, fit_intercept=True,\n",
      "                                              l1_ratio=0.15,\n",
      "                                              learning_rate='optimal',\n",
      "                                              loss='hinge', max_iter=1000,\n",
      "                                              n_iter_no_change=5, n_jobs=None,\n",
      "                                              penalty='l2', power_t=0.5,\n",
      "                                              random_state=42, shuffle=True,\n",
      "                                              tol=0.001,\n",
      "                                              validation_fraction=0.1,\n",
      "                                              verbose=0, warm_start=False),\n",
      "                      n_jobs=None)} 0.2743181384703414\n"
     ]
    }
   ],
   "source": [
    "print(cv.best_params_, cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 related\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.35      0.47      1266\n",
      "           1       0.81      0.96      0.88      3938\n",
      "           2       0.00      0.00      0.00        40\n",
      "\n",
      "    accuracy                           0.80      5244\n",
      "   macro avg       0.52      0.44      0.45      5244\n",
      "weighted avg       0.79      0.80      0.78      5244\n",
      "\n",
      "1 request\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94      4349\n",
      "           1       0.79      0.56      0.66       895\n",
      "\n",
      "    accuracy                           0.90      5244\n",
      "   macro avg       0.85      0.77      0.80      5244\n",
      "weighted avg       0.89      0.90      0.89      5244\n",
      "\n",
      "2 offer\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5218\n",
      "           1       0.00      0.00      0.00        26\n",
      "\n",
      "    accuracy                           1.00      5244\n",
      "   macro avg       0.50      0.50      0.50      5244\n",
      "weighted avg       0.99      1.00      0.99      5244\n",
      "\n",
      "3 aid_related\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82      3113\n",
      "           1       0.74      0.72      0.73      2131\n",
      "\n",
      "    accuracy                           0.79      5244\n",
      "   macro avg       0.78      0.78      0.78      5244\n",
      "weighted avg       0.78      0.79      0.78      5244\n",
      "\n",
      "4 medical_help\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      4822\n",
      "           1       0.73      0.13      0.22       422\n",
      "\n",
      "    accuracy                           0.93      5244\n",
      "   macro avg       0.83      0.56      0.59      5244\n",
      "weighted avg       0.91      0.93      0.90      5244\n",
      "\n",
      "5 medical_products\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      4974\n",
      "           1       0.74      0.19      0.30       270\n",
      "\n",
      "    accuracy                           0.95      5244\n",
      "   macro avg       0.85      0.59      0.64      5244\n",
      "weighted avg       0.95      0.95      0.94      5244\n",
      "\n",
      "6 search_and_rescue\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      5117\n",
      "           1       0.77      0.08      0.14       127\n",
      "\n",
      "    accuracy                           0.98      5244\n",
      "   macro avg       0.87      0.54      0.57      5244\n",
      "weighted avg       0.97      0.98      0.97      5244\n",
      "\n",
      "7 security\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      5156\n",
      "           1       0.00      0.00      0.00        88\n",
      "\n",
      "    accuracy                           0.98      5244\n",
      "   macro avg       0.49      0.50      0.50      5244\n",
      "weighted avg       0.97      0.98      0.97      5244\n",
      "\n",
      "8 military\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      5089\n",
      "           1       0.44      0.05      0.08       155\n",
      "\n",
      "    accuracy                           0.97      5244\n",
      "   macro avg       0.70      0.52      0.53      5244\n",
      "weighted avg       0.96      0.97      0.96      5244\n",
      "\n",
      "9 water\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      4905\n",
      "           1       0.74      0.64      0.69       339\n",
      "\n",
      "    accuracy                           0.96      5244\n",
      "   macro avg       0.86      0.81      0.83      5244\n",
      "weighted avg       0.96      0.96      0.96      5244\n",
      "\n",
      "10 food\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97      4649\n",
      "           1       0.82      0.75      0.78       595\n",
      "\n",
      "    accuracy                           0.95      5244\n",
      "   macro avg       0.89      0.87      0.88      5244\n",
      "weighted avg       0.95      0.95      0.95      5244\n",
      "\n",
      "11 shelter\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97      4774\n",
      "           1       0.80      0.52      0.63       470\n",
      "\n",
      "    accuracy                           0.95      5244\n",
      "   macro avg       0.88      0.76      0.80      5244\n",
      "weighted avg       0.94      0.95      0.94      5244\n",
      "\n",
      "12 clothing\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      5171\n",
      "           1       0.85      0.32      0.46        73\n",
      "\n",
      "    accuracy                           0.99      5244\n",
      "   macro avg       0.92      0.66      0.73      5244\n",
      "weighted avg       0.99      0.99      0.99      5244\n",
      "\n",
      "13 money\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      5140\n",
      "           1       0.67      0.02      0.04       104\n",
      "\n",
      "    accuracy                           0.98      5244\n",
      "   macro avg       0.82      0.51      0.51      5244\n",
      "weighted avg       0.97      0.98      0.97      5244\n",
      "\n",
      "14 missing_people\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      5184\n",
      "           1       0.00      0.00      0.00        60\n",
      "\n",
      "    accuracy                           0.99      5244\n",
      "   macro avg       0.49      0.50      0.50      5244\n",
      "weighted avg       0.98      0.99      0.98      5244\n",
      "\n",
      "15 refugees\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      5073\n",
      "           1       0.82      0.05      0.10       171\n",
      "\n",
      "    accuracy                           0.97      5244\n",
      "   macro avg       0.89      0.53      0.54      5244\n",
      "weighted avg       0.96      0.97      0.96      5244\n",
      "\n",
      "16 death\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      5007\n",
      "           1       0.90      0.38      0.53       237\n",
      "\n",
      "    accuracy                           0.97      5244\n",
      "   macro avg       0.94      0.69      0.76      5244\n",
      "weighted avg       0.97      0.97      0.96      5244\n",
      "\n",
      "17 other_aid\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93      4549\n",
      "           1       0.74      0.02      0.04       695\n",
      "\n",
      "    accuracy                           0.87      5244\n",
      "   macro avg       0.80      0.51      0.48      5244\n",
      "weighted avg       0.85      0.87      0.81      5244\n",
      "\n",
      "18 infrastructure_related\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97      4916\n",
      "           1       0.00      0.00      0.00       328\n",
      "\n",
      "    accuracy                           0.94      5244\n",
      "   macro avg       0.47      0.50      0.48      5244\n",
      "weighted avg       0.88      0.94      0.91      5244\n",
      "\n",
      "19 transport\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      5004\n",
      "           1       0.67      0.06      0.11       240\n",
      "\n",
      "    accuracy                           0.96      5244\n",
      "   macro avg       0.81      0.53      0.54      5244\n",
      "weighted avg       0.94      0.96      0.94      5244\n",
      "\n",
      "20 buildings\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      4977\n",
      "           1       0.84      0.19      0.32       267\n",
      "\n",
      "    accuracy                           0.96      5244\n",
      "   macro avg       0.90      0.60      0.65      5244\n",
      "weighted avg       0.95      0.96      0.94      5244\n",
      "\n",
      "21 electricity\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      5122\n",
      "           1       0.80      0.03      0.06       122\n",
      "\n",
      "    accuracy                           0.98      5244\n",
      "   macro avg       0.89      0.52      0.53      5244\n",
      "weighted avg       0.97      0.98      0.97      5244\n",
      "\n",
      "22 tools\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      5212\n",
      "           1       0.00      0.00      0.00        32\n",
      "\n",
      "    accuracy                           0.99      5244\n",
      "   macro avg       0.50      0.50      0.50      5244\n",
      "weighted avg       0.99      0.99      0.99      5244\n",
      "\n",
      "23 hospitals\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      5198\n",
      "           1       0.00      0.00      0.00        46\n",
      "\n",
      "    accuracy                           0.99      5244\n",
      "   macro avg       0.50      0.50      0.50      5244\n",
      "weighted avg       0.98      0.99      0.99      5244\n",
      "\n",
      "24 shops\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5222\n",
      "           1       0.00      0.00      0.00        22\n",
      "\n",
      "    accuracy                           1.00      5244\n",
      "   macro avg       0.50      0.50      0.50      5244\n",
      "weighted avg       0.99      1.00      0.99      5244\n",
      "\n",
      "25 aid_centers\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      5177\n",
      "           1       0.00      0.00      0.00        67\n",
      "\n",
      "    accuracy                           0.99      5244\n",
      "   macro avg       0.49      0.50      0.50      5244\n",
      "weighted avg       0.97      0.99      0.98      5244\n",
      "\n",
      "26 other_infrastructure\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      5021\n",
      "           1       0.00      0.00      0.00       223\n",
      "\n",
      "    accuracy                           0.96      5244\n",
      "   macro avg       0.48      0.50      0.49      5244\n",
      "weighted avg       0.92      0.96      0.94      5244\n",
      "\n",
      "27 weather_related\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92      3806\n",
      "           1       0.84      0.72      0.78      1438\n",
      "\n",
      "    accuracy                           0.89      5244\n",
      "   macro avg       0.87      0.84      0.85      5244\n",
      "weighted avg       0.88      0.89      0.88      5244\n",
      "\n",
      "28 floods\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      4833\n",
      "           1       0.91      0.52      0.66       411\n",
      "\n",
      "    accuracy                           0.96      5244\n",
      "   macro avg       0.94      0.76      0.82      5244\n",
      "weighted avg       0.96      0.96      0.95      5244\n",
      "\n",
      "29 storm\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97      4758\n",
      "           1       0.73      0.60      0.66       486\n",
      "\n",
      "    accuracy                           0.94      5244\n",
      "   macro avg       0.84      0.79      0.81      5244\n",
      "weighted avg       0.94      0.94      0.94      5244\n",
      "\n",
      "30 fire\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      5191\n",
      "           1       0.33      0.02      0.04        53\n",
      "\n",
      "    accuracy                           0.99      5244\n",
      "   macro avg       0.66      0.51      0.52      5244\n",
      "weighted avg       0.98      0.99      0.99      5244\n",
      "\n",
      "31 earthquake\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      4766\n",
      "           1       0.88      0.80      0.84       478\n",
      "\n",
      "    accuracy                           0.97      5244\n",
      "   macro avg       0.93      0.89      0.91      5244\n",
      "weighted avg       0.97      0.97      0.97      5244\n",
      "\n",
      "32 cold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kbosko/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      5127\n",
      "           1       0.76      0.14      0.23       117\n",
      "\n",
      "    accuracy                           0.98      5244\n",
      "   macro avg       0.87      0.57      0.61      5244\n",
      "weighted avg       0.98      0.98      0.97      5244\n",
      "\n",
      "33 other_weather\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      4968\n",
      "           1       0.67      0.01      0.01       276\n",
      "\n",
      "    accuracy                           0.95      5244\n",
      "   macro avg       0.81      0.50      0.49      5244\n",
      "weighted avg       0.93      0.95      0.92      5244\n",
      "\n",
      "34 direct_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.92      4223\n",
      "           1       0.74      0.42      0.53      1021\n",
      "\n",
      "    accuracy                           0.86      5244\n",
      "   macro avg       0.81      0.69      0.72      5244\n",
      "weighted avg       0.85      0.86      0.84      5244\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, col in enumerate(Y_nochildalone.columns):\n",
    "    print(i, col)\n",
    "    print(classification_report(Y_test_nca.to_numpy()[:, i], Y_pred[:, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifying the best parameters that were determined in the previous gridsearch step\n",
    "pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize, max_df=0.5, ngram_range=(1, 2), stop_words='english')),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(SGDClassifier(random_state = 42)))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('model.p.gz', 'wb') as gzipped_f:\n",
    "    # Pickle the trained pipeline using the highest protocol available.\n",
    "    pickled = pickle.dumps(pipeline)\n",
    "    gzipped_f.write(pickled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below is the code I tried with FeatureUnion to add information from 'genre' column. The code works. \n",
    "#However, I realized that I won't be able to automate message categorization as it is currently implemented in the app\n",
    "#because I will need to specify genre next to the message as input (--> redesign the app)\n",
    "#Something to try in the future!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load data from database\n",
    "# engine = create_engine('sqlite:///data/DisasterResponse.db')\n",
    "# df = pd.read_sql(\"SELECT * FROM DisasterResponse\", engine)\n",
    "# X = df[['message', 'genre']]\n",
    "# Y = df.iloc[:, 4:]\n",
    "# category_names = df.columns[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.base import TransformerMixin\n",
    "\n",
    "# class DataFrameColumnExtracter(TransformerMixin):\n",
    "\n",
    "#     def __init__(self, column):\n",
    "#         self.column = column\n",
    "\n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X, y=None):\n",
    "#         return X[self.column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FeatureReshaper(TransformerMixin):\n",
    "\n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X, y=None):\n",
    "#         return X.to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# nlp_pipe = make_pipeline(\n",
    "#     DataFrameColumnExtracter('message'), \n",
    "#     CountVectorizer(tokenizer=tokenize),\n",
    "#     TfidfTransformer(),\n",
    "# )\n",
    "\n",
    "# dummy_pipe = make_pipeline(\n",
    "#     DataFrameColumnExtracter('genre'),\n",
    "#     FeatureReshaper(),\n",
    "#     OneHotEncoder()\n",
    "# )\n",
    "\n",
    "# feature_union = make_union(nlp_pipe, dummy_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = Pipeline([\n",
    "#     ('features', feature_union),\n",
    "#     ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
